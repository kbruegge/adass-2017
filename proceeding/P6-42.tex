% This is the ADASS_template.tex LaTeX file, 26th August 2016.
% It is based on the ASP general author template file, but modified to reflect the specific
% requirements of the ADASS proceedings.
% Copyright 2014, Astronomical Society of the Pacific Conference Series
% Revision:  14 August 2014


\documentclass[11pt,twoside]{article}

% Do not use packages other than asp2014.
\usepackage{asp2014}

\aspSuppressVolSlug
\resetcounters

% References must all use BibTeX entries in a .bibfile.
% References must be cited in the text using \citep{} or \citep{}.
% Do not use \cite{}.
% See ManuscriptInstructions.pdf for more details
\bibliographystyle{asp2014}

% 1 author: "Surname"
% 2 authors: "Surname1 and Surname2"
% 3 authors: "Surname1, Surname2, and Surname3"
% >3 authors: "Surname1 et al."
% Use mixed case type for the shortened title
% Ensure shortened title does not cause an overfull hbox LaTeX error
% See ASPmanual2010.pdf 2.1.4  and ManuscriptInstructions.pdf for more details
\markboth{Br\"ugge et al.}{Distributed Real-Time Data Stream Analysis for CTA}

\begin{document}

\title{Distributed real-time data stream analysis for CTA}

% Note the position of the comma between the author name and the
% affiliation number.
% Author names should be separated by commas.
% The final author should be preceded by "and".
% Affiliations should not be repeated across multiple \affil commands. If several
% authors share an affiliation this should be in a single \affil which can then
% be referenced for several author names.
% See ManuscriptInstructions.pdf and ASPmanual2010.pdf 3.1.4 for more details
\author{Kai Br\"ugge$^1$  and Alexey Egorov$^2$}
\affil{$^1$TU Dortmund, Dortmund, Germany; \email{kai.bruegge@tu-dortmund.de}}
\affil{$^2$TU Dortmund, Dortmund, Dortmund; \email{alexey.egorov@tu-dortmund.de}}


% This section is for ADS Processing.  There must be one line per author.
\paperauthor{Kai~Br√ºgge}{kai.bruegge@tu-dortmund.de}{}{TU Dortmund}{Experimental Physics 5}{Dortmund}{NRW}{44227}{Germany}
\paperauthor{Alexey~Egorov}{alexey.egorov@tu-dortmund.de}{}{TU Dortmund}{Artificial Intelligence Group}{Dortmund}{NRW}{44227}{Germany}

\begin{abstract}

  Once completed, the  Cherenkov Telescope Array (CTA)  will be able to map the gamma-ray sky in a wide energy range from several tens of GeV to some hundreds of TeV and will be more sensitive than previous experiments by an order of magnitude.
  It opens up the opportunity to observe transient phenomena like gamma-ray bursts (GRBs) and flaring active galactic nuclei (AGN).  In order to successfully trigger multi-wavelengths observations of transients, CTA has to be able to alert other observatories as quickly as possible.  Multi wavelength observations are essential for gaining insights into the processes occurring within these sources of such high energy radiation.

  CTA will consist of approximately 100 telescopes of different sizes and designs.
  Images are streamed from all the telescopes into a central computing facility on site.
  During observation CTA will produce a stream of up to 15 000 images per second. Noise suppression and feature extraction algorithms are applied to each image in the stream as well as previously trained machine learning models.
  Restricted computing power of a single machine and the limits of network's data transfer rates become a bottleneck for stream processing systems in a traditional single-machine setting.
  We explore several different distributed streaming technologies from the Apache Big-Data eco-system like Spark, Flink, Storm to handle the large amount of data coming from the telescopes.
  To share a single code base while executing on different streaming engines we employ  abstraction layers such as the streams-framework and the Apache Beam project.
  These use  a high level language to build up processing pipelines that can transformed into native the pipelines of the different platforms.
  Here we present results of our investigation and show a first prototype capable of analyzing CTA data in real-time.

\end{abstract}

\section{The Cherenkov Telecope Array}
Once completed, the  Cherenkov Telescope Array (CTA)  will be able to map the gamma-ray sky
in a wide energy range from several tens of GeV to some hundreds of TeV and will be more sensitive
than previous experiments by an order of magnitude.
CTA will consist of approximately 100 imaging air cherenkov telescopes (IACTs) of different sizes and designs.
Telescope data will be streamed via network from the telescopes to a central computing facility
on-site.
Cherenkov telescopes record light produced by particle showers induced in the upper atmosphere by cosmic rays.
While the cosmic ray flux is approximately isotropic over the sky, gamma rays of cosmic origin can be pinpointed back to its source.
Filtering air showers produced by cosmic rays while keeping those produced by gamma rays is a big in IACT data analysis.



\section{Real Time Analysis}

One of CTA's main goals is monitoring the sky for transient events. These include Gamma-Ray Burst (GRBs) events and
Active Galactic Nuclei (AGNs).
To gain more understanding about the physics behind GRBs and AGN it is vital to perform multi-wavelengths observations.
In case a GRB is detected,  CTA can alert other experiments to trigger observations in other wavelengths bands.
The data aquisition system will supply the real time analysis (RTA) with calibrated images of each triggered telescope.
The expected event rate of CTA is between $10000$ and $20000$ events per second depending on deployment~\citep{trigger}.
Features for classication/regression are calculated on each image in the triggered events.
Simulated and labeled data is used to train the models using the Python machine learning library scikit-learn~\citep{sklearn}.
These models are then used for filtering of comsic ray showers and estimation of primary particle energy.
The trained scikit-learn models are converted into the PMML~\citep{pmml} format using the sklearn2pmml~\citep{sklearn2pmml} library.
This way the stored model can be shared between programming languages and applied to the data stream from the telescopes.

\articlefiguretwo{P6-42_f1.eps}{P6-42_f2.eps}{fig:image}{\emph{Left:} A typical image as recorded by an IACT camera.
\emph{Right:} Distribution of trigger multiplicities in the simulated data.}

\section{Runtime Measurements}

Frameworks for distributed stream processing such as Apache Storm \cite{storm}, Apache Flink \cite{flink} or Apache Spark \cite{spark} allow for simple and maintable workload distribution with fault tolerance and high availability mechanisms to recover from hardware, network or further failures. 
Tuning Apache Spark for fast streaming applications is more complicated due to its micro-batch architecture. 
Hence, we compare the runtime of Storm and Flink on a single core to measure the impact of the overhead these frameworks produce. 
% and the streams-framework \cite{streams}
To share a single code base while executing on different streaming engines we employ the streams-framework \cite{streams} as an abstraction layer.
This way the analysis pipeline is only defined once, but can be easily executed on top of different distributed streaming engines. 
%the Apache Beam project.
Figure~\ref{fig:dist-frameworks} shows no clear preference between Storm and Flink when it comes to runtime overhead. 
The reduction in throughput is negligible compared to multiple features of these frameworks.
We continue our experiments on Flink due to easier setup, better visualization tools and a more comfortable abstraction compared to the rather low-level API of Storm.

As the expected eventrate of CTA is between $10000$ and $20000$ events per second depending on deployment \cite{trigger}, the requirement for the real-time processing system is to be able to analyse such a thoughput of data. 
The right Figure~\ref{fig:dist-frameworks} presents the evaluation of an examplary CTA pipeline executed on top of Flink.
For this test a machine with $24$ physical CPU cores and $128$ Gigabytes of RAM was used.
%The  gure on the left shows the number of events per second versus the number of used CPU cores. 
The datarate goes up to approximately $14000$ events per second. 
This shows that it will be possible to completely analyse CTA data with only a few of computers.

%Tuning the Apache Spark framework for fast streaming application is more complicated due to its micro-batch architecture.
%Hence, we concentrated on Apache Storm\cite{storm} and Apache Flink\cite{flink}. We compare the runtime of Storm, Flink and the streams-framework\cite{streams} in
%a local setup on a single core to measure the impact of the overhead these frameworks produce.
%The results can be seen on the image on the left.
%There is no clear preference between Storm and Flink when it comes to runtime overhead. We chose to continue our experiments on
%Apache Flink due to easier setup, better visualization tools and a more comfortable abstraction compared to the rather low-level
%API of Apache Storm.

\articlefiguretwo{P6-42_f4.eps}{P6-42_f3.eps}{fig:dist-frameworks}{\emph{Left:} Overhead produced by distributed processing frameworks. 
\emph{Right:} Throughput of CTA analysis pipeline executed on multiple cores.}


\section{Machine Learning Performance}

We use a Random Forest classifier to separte signal events from background events. We trained an ensemble of 200 trees
on simulated data. The trained model is then applied to each single telescope in the data stream. The predictions
for each telescope are then averaged to get a combined prediction for the entire event.

\articlefiguretwo{P6-42_f5.eps}{P6-42_f6.eps}{fig:image}{\emph{Left:} Performance of background suppression for a single 
telescope.
\emph{Right:} Improved background suppression of a combined predictor.}




\bibliography{references}

\end{document}
